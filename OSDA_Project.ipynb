{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Datasets information\n",
    "**Dataset**: 1   \n",
    "**Name**: \"Congressional Voting Records Data Set\"  \n",
    "**Link**: https://archive.ics.uci.edu/ml/datasets/congressional+voting+records    \n",
    "**Number of instances**: 435  \n",
    "**Number of attributes**: 16  \n",
    "**Missing values**: Yes  \n",
    "\n",
    "**Dataset**: 2   \n",
    "**Name**: \"Tic-Tac-Toe Endgame Data Set\"  \n",
    "**Link**: https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe+Endgame  \n",
    "**Number of instances**: 958     \n",
    "**Number of attributes**: 27   \n",
    "**Missing values**: No  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Datasets standartization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.1 Importing datasets**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Congressional Voting Records Data Set\n",
    "df_1 = pd.read_csv(\n",
    "    filepath_or_buffer=\"http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\", \n",
    "    header=None, \n",
    "    sep=',')\n",
    "headers_1 = ['target', 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "df_1.columns= headers_1\n",
    "\n",
    "# Tic-Tac-Toe Endgame Data Set\n",
    "df_2 = pd.read_csv(\n",
    "    filepath_or_buffer=\"https://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\", \n",
    "    header=None, \n",
    "    sep=',')\n",
    "headers_2 = [1,2,3,4,5,6,7,8,9,'target']\n",
    "df_2.columns= headers_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.2 Checking unprepared data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df_1.info())\n",
    "print(df_2.info())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3 Preparation of Voting records dataset.**  \n",
    "Changing string values to variables and deleting rows with NaNs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_1 = df_1.replace('?', np.nan)\n",
    "df_1 = df_1.replace('y', True)\n",
    "df_1 = df_1.replace('n', False)\n",
    "df_1 = df_1.dropna().reset_index(drop=True)\n",
    "df_1 = df_1.replace('democrat', True)\n",
    "df_1 = df_1.replace('republican', False)\n",
    "df_1 = df_1[list(df_1.columns.values)].astype('bool')\n",
    "df_1.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.4 Preparation of Tic-Tac-Toe Dataset**  \n",
    "Getting dummy features from all features and changing string values to bool variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2 = df_2.replace('positive', True)\n",
    "df_2 = df_2.replace('negative', False)\n",
    "df_2 = pd.get_dummies(df_2, columns=[1,2,3,4,5,6,7,8,9])\n",
    "df_2 = df_2[list(df_2.columns.values)].astype('bool')\n",
    "df_2.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.5 Saving standartized datasets**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = r'C:/Users/Hp/PycharmProjects/OSDA/std_datasets/'\n",
    "df_1.to_csv(os.path.join(directory, r'hv.csv'), index=False)\n",
    "df_2.to_csv(os.path.join(directory, r'ttt.csv'), index=False)\n",
    "\n",
    "# if using .py script\n",
    "# script_dir = os.path.abspath(os.path.dirname(sys.argv[0]) or '.') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Supporting functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.1 Cross Validation function**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "import sys\n",
    "\n",
    "def cross_validation(path_in, path_out, dataset_short_name, n_splits):\n",
    "    \"\"\" Makes cross validation of dataset from selected path in N splits and saves to selected path. \n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_in : str\n",
    "        The path to .csv file with dataset to pass to function\n",
    "    path_out : str\n",
    "        The path to the directory where cross validated dataset will be saved\n",
    "    dataset_short_name : str\n",
    "        The short name of the dataset\n",
    "    n_splits : int\n",
    "        The number of KFolds splits for cross validation\n",
    "    \"\"\"\n",
    "    \n",
    "    df_prep = pd.read_csv(path_in)\n",
    "    kf = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=None)\n",
    "    kf.get_n_splits(df_prep)\n",
    "    k = 1\n",
    "    \n",
    "    for train_index, test_index in kf.split(df_prep):\n",
    "        df_prep.iloc[train_index].to_csv(os.path.join(path_out, dataset_short_name+'_train_'+str(k)+'.csv'),index=False)\n",
    "        df_prep.iloc[test_index].to_csv(os.path.join(path_out, dataset_short_name+'_test_'+str(k)+'.csv'),index=False)\n",
    "        k += 1\n",
    "        \n",
    "    return 'Cross Validation Completed'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.2 Data preprocessing**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_preprocessing(path_train, path_test, target_column_name = 'target'):\n",
    "    \"\"\" Makes plus, minus contexts of selected train dataset in list of dictionaries format \n",
    "    and transforms test dataset in list of dictionaries w/o target feature and target feature list.\n",
    "    \n",
    "    If the argument 'target_column_name' isn't passed in, the default 'target' name is used.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_train : str\n",
    "        The path to @_train#.csv where # is a number of file and @ is a shortname of dataset\n",
    "    path_test : str\n",
    "        The path to @_test#.csv where # is a number of file and @ is a shortname of dataset\n",
    "    target_column_name : str\n",
    "        The name of target feature in selected dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    train = pd.read_csv(path_train)\n",
    "    test = pd.read_csv(path_test)\n",
    "\n",
    "    plus_context = train[train[target_column_name] == True]\n",
    "    minus_context = train[train[target_column_name] == False]\n",
    "\n",
    "    X_plus = plus_context.drop(target_column_name, axis = 1).to_dict('records')\n",
    "    X_minus = minus_context.drop(target_column_name, axis = 1).to_dict('records')\n",
    "    X_test = test.drop(target_column_name, axis = 1).to_dict('records')\n",
    "    y_test = test[target_column_name].tolist()\n",
    "    \n",
    "    return X_plus, X_minus, X_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.3 Metrics evaluation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def metrics_test(y_real_list, y_pred_list):\n",
    "    \"\"\" Counts  sklearn.metrics (Accuracy, ROC AUC, Precision and Recall) for real target and its predicted values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_real_list : list\n",
    "        The real target values of dataset\n",
    "    y_pred_list : list\n",
    "        The predicted target values of dataset via some algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    y_real = np.array(y_real_list)\n",
    "    y_pred = np.array(y_pred_list)\n",
    "    \n",
    "    acc = accuracy_score(y_real, y_pred)\n",
    "    roc_auc = roc_auc_score(y_real, y_pred)\n",
    "    prc = precision_score(y_real, y_pred)\n",
    "    rcl = recall_score(y_real, y_pred)\n",
    "    \n",
    "#     print('Accuracy score: {:.4f}'.format(acc))\n",
    "#     print('Roc AUC Score: {:.4f}'.format(roc_auc))\n",
    "#     print('Precision Score: {:.4f}'.format(prc))\n",
    "#     print('Recall Score: {:.4f}'.format(rcl))\n",
    "    \n",
    "    return acc, roc_auc, prc, rcl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.4 Supporting functions for algorithms**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dict_intersec(dict1, dict2, option='intersec'):\n",
    "    \"\"\"Gets dictionary of intersection or bool of subset  of two dictionaries depending on 'option'\n",
    "    \n",
    "    If the argument 'option' isn't passed in, the default 'intersec' option is used.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dict1 : dict\n",
    "        The first dictionary. Its length is less or equal than the length of the second dictionary\n",
    "    dict2 : dict\n",
    "        The second dictionary\n",
    "    option : str\n",
    "        Option 'intersec' or 'subset' \n",
    "    \"\"\"\n",
    "\n",
    "    if option == 'intersec':\n",
    "        return dict(set(dict1.items()) & set(dict2.items()))\n",
    "    if option == 'subset':\n",
    "        return dict1.items() <= dict2.items()\n",
    "    else:\n",
    "        raise ValueError('Unknown option was used.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algorithm 1\n",
    "Algorithm based on voting system, where eveery plus context object votes for plus classification, if its intersection with test object is not a subset(depending on thresholds) of of minus context and vice versa.  \n",
    "Test object classifies positive, if there is more votes for plus classification (and vice versa).  \n",
    "There is one threshold, which controls, how many subsets of plus/minus and test sets intersection can be in minus/plus context."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def Alg_1(X_plus, X_minus, X_test, threshold_subset):\n",
    "    \"\"\"Algorithm based on voting system, where every plus context object votes for plus classification, \n",
    "    if its intersection with test object is not a subset(depending on threshold) of of minus context object and vice versa.\n",
    "    Test object classifies positive, if there is more votes for plus classification (and vice versa).\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_plus : list of dict\n",
    "        The plus context of train dataset\n",
    "    X_minus : list of dict\n",
    "        The minus context of train dataset\n",
    "    X_test : list of dict\n",
    "        The test dataset\n",
    "    threshold_subset : int\n",
    "        The limitation threshold for counter of subsets of plus/minus and test sets intersection in minus/plus context \n",
    "    \"\"\"\n",
    "    \n",
    "    norm_plus = (len(X_plus)+len(X_minus))//len(X_plus)\n",
    "    norm_minus = (len(X_plus)+len(X_minus))//len(X_minus)\n",
    "    y_pred = []\n",
    "    random_counter = 0\n",
    "    \n",
    "    for ent_test in X_test:\n",
    "        labels = {'plus':0, 'minus':0}\n",
    "        for ent_plus in X_plus:\n",
    "            counter_plus = 0 \n",
    "            intersec_plus = dict_intersec(ent_test, ent_plus, option='intersec')\n",
    "            for ent_minus in X_minus:\n",
    "                if dict_intersec(intersec_plus, ent_minus, option='subset'):\n",
    "                    counter_plus += 1\n",
    "            if counter_plus <= threshold_subset:\n",
    "                labels['plus'] += 1\n",
    "        for ent_minus in X_minus:\n",
    "            counter_minus = 0\n",
    "            intersec_minus = dict_intersec(ent_test, ent_minus, option='intersec')\n",
    "            for ent_plus in X_plus:\n",
    "                if dict_intersec(intersec_minus, ent_plus, option='subset'):\n",
    "                    counter_minus += 1\n",
    "            if counter_minus <= threshold_subset:\n",
    "                labels['minus'] += 1\n",
    "        \n",
    "        labels['plus'] = labels['plus']*norm_plus\n",
    "        labels['minus'] = labels['minus']*norm_minus\n",
    "        if labels['plus'] > labels['minus']:\n",
    "            y_pred.append(True)\n",
    "        elif labels['plus'] < labels['minus']:\n",
    "            y_pred.append(False)\n",
    "        else:\n",
    "            random_counter += 1\n",
    "            random.seed(1)\n",
    "            y_pred.append(bool(random.getrandbits(1)))\n",
    "            \n",
    "#         if random_counter >= 0.25*len(X_test):\n",
    "#             raise ValueError('Answer is too random')\n",
    "    \n",
    "    randomness = random_counter/len(X_test)\n",
    "    \n",
    "    return y_pred,randomness"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algorithm 2\n",
    "Algorithm based on voting system, where every plus context object votes for plus classification, if there is more its intersections (depending on threshold) with test object than with minus context.  \n",
    "Test object classifies positive, if there is more votes for plus classification (and vice versa).   \n",
    "It has threshold_intersec, which controls, how many elements can be in plus/minus context and test sets intersection.  \n",
    "It is more simple version of Algorithm 1, because it doesn't search subsets in another context."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def Alg_2(X_plus, X_minus, X_test, threshold_intersec):\n",
    "    \"\"\"Algorithm based on voting system, where every plus context object votes for plus classification, \n",
    "    if there is more its intersections (depending on threshold) with test object than with minus context object.  \n",
    "    Test object classifies positive, if there is more votes for plus classification (and vice versa).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_plus : list of dict\n",
    "        The plus context of train dataset\n",
    "    X_minus : list of dict\n",
    "        The minus context of train dataset\n",
    "    X_test : list of dict\n",
    "        The test dataset \n",
    "    threshold_intersec : float\n",
    "        The limitation threshold for counter of elements in plus/minus context and test sets intersection\n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    y_pred = []\n",
    "    random_counter = 0\n",
    "    \n",
    "    for ent_test in X_test:\n",
    "\n",
    "        labels = {'plus' : 0, 'minus' : 0}\n",
    "        \n",
    "        for ent_plus in X_plus:\n",
    "            counter_plus = 0 \n",
    "            intersec_plus = dict_intersec(ent_test, ent_plus, option='intersec')\n",
    "            intersec_plus_len_norm = len(intersec_plus)/len(ent_test)\n",
    "            if intersec_plus_len_norm > threshold_intersec:\n",
    "                labels['plus'] += 1\n",
    "   \n",
    "        for ent_minus in X_minus:\n",
    "            counter_minus = 0\n",
    "            intersec_minus = dict_intersec(ent_test, ent_minus, option='intersec')\n",
    "            intersec_minus_len_norm = len(intersec_minus)/len(ent_test)\n",
    "            if intersec_minus_len_norm > threshold_intersec:\n",
    "                labels['minus'] += 1\n",
    "                \n",
    "        if labels['plus'] > labels['minus']:\n",
    "            y_pred.append(True)\n",
    "        elif labels['plus'] < labels['minus']:\n",
    "            y_pred.append(False)\n",
    "        else:\n",
    "            random_counter += 1\n",
    "            random.seed(1)\n",
    "            y_pred.append(bool(random.getrandbits(1)))\n",
    "            \n",
    "#         if random_counter >= 0.25*len(X_test):\n",
    "#             raise ValueError('Answer is too random')\n",
    "    \n",
    "    randomness = random_counter/len(X_test)\n",
    "    \n",
    "    return y_pred, randomness"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algorithm 3\n",
    "Algorithm based on voting system, where eveery plus context object votes for plus classification, if its intersection(depending on threshold) with test object is not a subset(depending on threshold) of of minus context and vice versa.\n",
    "Test set is True, if there is more votes for plus classification (and vice versa).\n",
    "\n",
    "This algorithm is the same approach, as algorithm 1, but with modification. It has threshold_intersec, which controls, how many elements can be in plus/minus context and test sets intersection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Alg_3(X_plus, X_minus, X_test, threshold_subset, threshold_intersec):\n",
    "    \"\"\"Algorithm based on voting system, where eveery plus context object votes for plus classification, \n",
    "    if its intersection(depending on threshold) with test object is not a subset(depending on threshold) \n",
    "    of minus context object and vice versa.\n",
    "    Test object classifies positive, if there is more votes for plus classification (and vice versa).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_plus : list of dict\n",
    "        The plus context of train dataset\n",
    "    X_minus : list of dict\n",
    "        The minus context of train dataset\n",
    "    X_test : list of dict\n",
    "        The test dataset\n",
    "    threshold_subset : int\n",
    "        The limitation threshold for counter of subsets of plus/minus context and test sets intersection in minus/plus context \n",
    "    threshold_intersec : float\n",
    "        The limitation threshold for counter of elements in plus/minus context and test sets intersection\n",
    "    \"\"\"\n",
    "    \n",
    "    norm_plus = (len(X_plus)+len(X_minus))//len(X_plus)\n",
    "    norm_minus = (len(X_plus)+len(X_minus))//len(X_minus)\n",
    "    y_pred = []\n",
    "    random_counter = 0\n",
    "    \n",
    "    for ent_test in X_test:\n",
    "        labels = {'plus':0, 'minus':0}\n",
    "        for ent_plus in X_plus:\n",
    "            counter_plus = 0 \n",
    "            intersec_plus = dict_intersec(ent_test, ent_plus, option='intersec')\n",
    "            intersec_plus_len_norm = len(intersec_plus)/len(ent_test)\n",
    "            if intersec_plus_len_norm >= threshold_intersec:\n",
    "                for ent_minus in X_minus:\n",
    "                    if dict_intersec(intersec_plus, ent_minus, option='subset'):\n",
    "                        counter_plus += 1\n",
    "                if counter_plus <= threshold_subset:\n",
    "                    labels['plus'] += 1\n",
    "        for ent_minus in X_minus:\n",
    "            counter_minus = 0\n",
    "            intersec_minus = dict_intersec(ent_test, ent_minus, option='intersec')\n",
    "            intersec_minus_len_norm = len(intersec_minus)/len(ent_test)\n",
    "            if intersec_minus_len_norm >= threshold_intersec:\n",
    "                for ent_plus in X_plus:\n",
    "                    if dict_intersec(intersec_minus, ent_plus, option='subset'):\n",
    "                        counter_minus += 1\n",
    "                if counter_minus <= threshold_subset:\n",
    "                    labels['minus'] += 1\n",
    "        \n",
    "        labels['plus'] = labels['plus']*norm_plus\n",
    "        labels['minus'] = labels['minus']*norm_minus\n",
    "        if labels['plus'] > labels['minus']:\n",
    "            y_pred.append(True)\n",
    "        elif labels['plus'] < labels['minus']:\n",
    "            y_pred.append(False)\n",
    "        else:\n",
    "            random_counter += 1\n",
    "            random.seed(1)\n",
    "            y_pred.append(bool(random.getrandbits(1)))\n",
    "            \n",
    "#         if random_counter >= 0.25*len(X_test):\n",
    "#             raise ValueError('Answer is too random')\n",
    "    \n",
    "    randomness = random_counter/len(X_test)\n",
    "    \n",
    "    return y_pred, randomness"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4\n",
    "\n",
    "Algorithm classifies test object by maximum subset of plus/minus context object and test object intersection in minus/plus context object.  \n",
    "Test object is True, if there is more votes for plus classification (and vice versa).  \n",
    "It has threshold_intersec, which controls, how many elements can be in plus/minus context and test sets intersection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def Alg_4(X_plus, X_minus, X_test, threshold_intersec):\n",
    "    \"\"\"Algorithm classifies test object by maximum subset of plus/minus context object and test object \n",
    "    intersection in minus/plus context object.\n",
    "    Test object is True, if there is more votes for plus classification (and vice versa).\n",
    "    It has threshold_intersec, which controls, how many elements can be in plus/minus context and test sets intersection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_plus : list of dict\n",
    "        The plus context of train dataset\n",
    "    X_minus : list of dict\n",
    "        The minus context of train dataset\n",
    "    X_test : list of dict\n",
    "        The test dataset\n",
    "    threshold_intersec : float\n",
    "        The limitation threshold for counter of elements in plus/minus context and test sets intersection\n",
    "    \"\"\"\n",
    "    \n",
    "    norm_plus = (len(X_plus)+len(X_minus))//len(X_plus)\n",
    "    norm_minus = (len(X_plus)+len(X_minus))//len(X_minus)\n",
    "    y_pred = []\n",
    "    random_counter = 0\n",
    "    \n",
    "    for ent_test in X_test:\n",
    "\n",
    "        max_counter_plus = 0\n",
    "        max_counter_minus = 0\n",
    "        \n",
    "        for ent_plus in X_plus:\n",
    "            counter_plus = 0 \n",
    "            \n",
    "            intersec_plus = dict_intersec(ent_test, ent_plus, option='intersec')\n",
    "            intersec_plus_len_norm = len(intersec_plus)/len(ent_test)\n",
    "            if intersec_plus_len_norm < threshold_intersec:\n",
    "                    continue\n",
    "            for ent_minus in X_minus:\n",
    "                if dict_intersec(intersec_plus, ent_minus, option='subset'):\n",
    "                    counter_plus += 1\n",
    "                    max_counter_plus = max(counter_plus, max_counter_plus)\n",
    "\n",
    "        for ent_minus in X_minus:\n",
    "            counter_minus = 0\n",
    "            intersec_minus = dict_intersec(ent_test, ent_minus, option='intersec')\n",
    "            intersec_minus_len_norm = len(intersec_minus)/len(ent_test)\n",
    "            if intersec_minus_len_norm < threshold_intersec:\n",
    "                    continue\n",
    "            for ent_plus in X_plus:\n",
    "                if dict_intersec(intersec_minus, ent_plus, option='subset'):\n",
    "                    counter_minus += 1\n",
    "                    max_counter_minus = max(counter_minus, max_counter_minus)\n",
    "                \n",
    "        y_pred.append(max_counter_plus<max_counter_minus)\n",
    "    \n",
    "    randomness = 0\n",
    "    \n",
    "    return y_pred, randomness"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BernoulliNB Algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "def bernoulliNB(dataset_shortname):\n",
    "    cv_path = 'C:/Users/Hp/PycharmProjects/OSDA/cross_validated/'\n",
    "    y_pred_arr = []\n",
    "    metrics = []\n",
    "    for i in range(1,11):\n",
    "        test = pd.read_csv(cv_path+dataset_shortname+r'_test_'+str(i)+r'.csv')\n",
    "        train = pd.read_csv(cv_path+dataset_shortname+r'_train_'+str(i)+r'.csv')\n",
    "        \n",
    "        X_train = train.drop('target', axis = 1).to_numpy()\n",
    "        y_train = train['target'].to_numpy()\n",
    "        \n",
    "        X_test = test.drop('target', axis = 1).to_numpy()\n",
    "        y_test = test['target'].to_numpy()\n",
    "        \n",
    "        clf = BernoulliNB()\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "#         print(y_test)\n",
    "        acc, roc_auc, prc, rcl = metrics_test(y_pred_list=y_pred, y_real_list=y_test.tolist())\n",
    "        metrics.append([acc, roc_auc, prc, rcl, 0])\n",
    "        y_pred_arr.append(y_pred)\n",
    "        \n",
    "    return y_pred_arr, y_test, metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Evaluating algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def alg_eval(dataset_shortname, alg_n=1, n_splits=10, is_already_cv=True, \n",
    "             threshold_subset=0, threshold_intersec=0, print_time=True):\n",
    "    \n",
    "    start=datetime.now()\n",
    "    \n",
    "    # vars\n",
    "    std_path = 'C:/Users/Hp/PycharmProjects/OSDA/std_datasets/'\n",
    "    cv_path = 'C:/Users/Hp/PycharmProjects/OSDA/cross_validated/'\n",
    "    # if using .py script\n",
    "    # script_dir = os.path.abspath(os.path.dirname(sys.argv[0]) or '.')\n",
    "    metrics = []\n",
    "    y_pred_arr = []\n",
    "    # 1. step - Cross Validation\n",
    "    if not is_already_cv:\n",
    "        cross_validation(path_in=std_path+dataset_shortname+r'.csv', #+r'tic_tac_toe.csv',\n",
    "                        path_out=cv_path,\n",
    "                        dataset_short_name=dataset_shortname,\n",
    "                        n_splits=n_splits)\n",
    "    for i in range(1,n_splits+1):\n",
    "        # 2. step - Data Preprocessing\n",
    "        X_plus, X_minus, X_test, y_test = data_preprocessing(path_test=cv_path+dataset_shortname+r'_test_'+str(i)+r'.csv',\n",
    "                                                             path_train=cv_path+dataset_shortname+r'_train_'+str(i)+r'.csv')\n",
    "\n",
    "        # 3. step - Algorithm\n",
    "        y_real = y_test\n",
    "        if alg_n == 1:\n",
    "            y_pred, rdnm = Alg_1(X_plus, X_minus, X_test, threshold_subset=threshold_subset)\n",
    "\n",
    "        if alg_n == 2:\n",
    "            y_pred, rdnm = Alg_2(X_plus, X_minus, X_test, threshold_intersec=threshold_intersec)\n",
    "\n",
    "        if alg_n == 3:\n",
    "            y_pred, rdnm = Alg_3(X_plus, X_minus, X_test, threshold_subset=threshold_subset, threshold_intersec=threshold_intersec)\n",
    "\n",
    "        if alg_n == 4:\n",
    "            y_pred, rdnm = Alg_4(X_plus, X_minus, X_test, threshold_intersec=threshold_intersec)\n",
    "\n",
    "        # 4. step - Metrics\n",
    "        acc, roc_auc, prc, rcl = metrics_test(y_pred_list=y_pred, y_real_list=y_real)\n",
    "        metrics.append([acc, roc_auc, prc, rcl, rdnm])\n",
    "        y_pred_arr.append(y_pred)\n",
    "    if print_time:\n",
    "        print ('Algorithm runtime: {}'.format(datetime.now()-start)) #str().split(\".\")[0]\n",
    "    return y_pred_arr, y_real, metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**6.1 Supporting functions**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def avg_metrics(metrics):\n",
    "    np_metrics = np.array(metrics)\n",
    "    return np.mean(np_metrics, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_metrics(metrics):\n",
    "    print('Accuracy score: {:.4f}'.format(metrics[0]))\n",
    "    print('Roc AUC Score: {:.4f}'.format(metrics[1]))\n",
    "    print('Precision Score: {:.4f}'.format(metrics[2]))\n",
    "    print('Recall Score: {:.4f}'.format(metrics[3]))\n",
    "    print('Randomness of prediction: {:.4f}'.format(metrics[4]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**6.2 Hyperparameters tuning**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def hp_tuning(dataset_shortname, alg_n):\n",
    "    avg_m_arr = np.empty((0,5), float)\n",
    "    if alg_n == 1:\n",
    "        for i in range(0,6):\n",
    "            _, _, metrics = alg_eval(dataset_shortname=dataset_shortname, alg_n=alg_n, threshold_subset=i, print_time=False)\n",
    "            avg_m = avg_metrics(metrics)\n",
    "            avg_m_arr = np.vstack([avg_m_arr, avg_m])\n",
    "    if alg_n in [2,3,4]:\n",
    "        for i in np.arange(0, 1.1, 0.1):\n",
    "            _, _, metrics = alg_eval(dataset_shortname=dataset_shortname, alg_n=alg_n, threshold_subset=0, threshold_intersec=i, print_time=False)\n",
    "            avg_m = avg_metrics(metrics)\n",
    "            avg_m_arr = np.vstack([avg_m_arr, avg_m])\n",
    "    \n",
    "    plt.plot(avg_m_arr)\n",
    "    plt.legend(['Accuracy score', 'ROC AUC', 'Precision','Recall','Prediction Randomness'],bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return avg_m_arr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_tuning(dataset_shortname='hv', alg_n=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_tuning(dataset_shortname='ttt', alg_n=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_tuning(dataset_shortname='hv', alg_n=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_tuning(dataset_shortname='ttt', alg_n=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_tuning(dataset_shortname='hv', alg_n=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_tuning(dataset_shortname='ttt', alg_n=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_tuning(dataset_shortname='hv', alg_n=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_tuning(dataset_shortname='ttt', alg_n=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**6.3 Hyperparameters results**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best hyperparameter for first algorithm is threshold_subset = 0 for both datasets.  \n",
    "Best hyperparameter for second algorithm is threshold_intersec = 0.7 and 0.8 for two datasets respectfully.  \n",
    "Best hyperparameter for third algorithm is threshold_subset = 0 (because it shows us on first alg) and threshold_intersec = 0.2 and 0.8 for two datasets respectfully.  \n",
    "Best hyperparameter for fourth algorithm is threshold_intersec = 0.4 and 0.7 for two datasets respectfully.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**6.4 Testing with tuned hyperparameters**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# alg 1 dataset 1 KFolds = 10\n",
    "y_pred, y_real, metrics = alg_eval(dataset_shortname='hv', alg_n=1, threshold_subset=0)\n",
    "avg_m_11 = avg_metrics(metrics)\n",
    "print_metrics(avg_m_11)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# alg 1 dataset 2 KFolds = 10\n",
    "y_pred, y_real, metrics = alg_eval(dataset_shortname='ttt', alg_n=1, threshold_subset=0)\n",
    "avg_m_12 = avg_metrics(metrics)\n",
    "print_metrics(avg_m_12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# alg 2 dataset 1 KFolds = 10\n",
    "y_pred, y_real, metrics = alg_eval(dataset_shortname='hv', alg_n=2, threshold_intersec=0.8)\n",
    "avg_m_21 = avg_metrics(metrics)\n",
    "print_metrics(avg_m_21)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# alg 2 dataset 2 KFolds = 10\n",
    "y_pred, y_real, metrics = alg_eval(dataset_shortname='ttt', alg_n=2, threshold_intersec=0.8)\n",
    "avg_m_22 = avg_metrics(metrics)\n",
    "print_metrics(avg_m_22)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# alg 3 dataset 1\n",
    "y_pred, y_real, metrics = alg_eval(dataset_shortname='hv', alg_n=3, threshold_subset=0, threshold_intersec=0.2)\n",
    "avg_m_31 = avg_metrics(metrics)\n",
    "print_metrics(avg_m_31)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# alg 3 dataset 2\n",
    "y_pred, y_real, metrics = alg_eval(dataset_shortname='ttt', alg_n=3, threshold_subset=0, threshold_intersec=0.5)\n",
    "avg_m_32 = avg_metrics(metrics)\n",
    "print_metrics(avg_m_32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# alg 4 dataset 1\n",
    "y_pred, y_real, metrics = alg_eval(dataset_shortname='hv', alg_n=4, threshold_intersec=0.5)\n",
    "avg_m_41 = avg_metrics(metrics)\n",
    "print_metrics(avg_m_41)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred, y_real, metrics = alg_eval(dataset_shortname='hv', alg_n=4, threshold_intersec=0.4)\n",
    "avg_m_41 = avg_metrics(metrics)\n",
    "print_metrics(avg_m_41)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# alg 4 dataset 2\n",
    "y_pred, y_real, metrics = alg_eval(dataset_shortname='ttt', alg_n=4, threshold_intersec=0.7)\n",
    "avg_m_42 = avg_metrics(metrics)\n",
    "print_metrics(avg_m_42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_metrics_hv = np.vstack([avg_m_11, avg_m_21,avg_m_31,avg_m_41])\n",
    "print(avg_metrics_hv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best: **Algorithm 4**  \n",
    "Algorithm runtime: 0:00:01.578181  \n",
    "Accuracy score: 0.9139  \n",
    "Roc AUC Score: 0.9173  \n",
    "Precision Score: 0.9779  \n",
    "Recall Score: 0.8664  \n",
    "Randomness of prediction: 0.0000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_metrics_ttt = np.vstack([avg_m_12, avg_m_22,avg_m_32,avg_m_42])\n",
    "print(avg_metrics_ttt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best: **Algorithm 2**  \n",
    "Algorithm runtime: 0:00:05.664447  \n",
    "Accuracy score: 0.9906  \n",
    "Roc AUC Score: 0.9873  \n",
    "Precision Score: 0.9853  \n",
    "Recall Score: 1.0000  \n",
    "Randomness of prediction: 0.0042"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Comparing to Bernoulli Naive Bayes**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred, y_real, metrics = bernoulliNB(dataset_shortname='hv')\n",
    "avg_m_bNB_hv = avg_metrics(metrics)\n",
    "print_metrics(avg_m_bNB_hv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred, y_real, metrics = bernoulliNB(dataset_shortname='ttt')\n",
    "avg_m_bNB_ttt = avg_metrics(metrics)\n",
    "print_metrics(avg_m_bNB_ttt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Histogram plots**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_metrics_ttt_full = np.vstack([avg_metrics_ttt, avg_m_bNB_ttt])\n",
    "avg_metrics_hv_full = np.vstack([avg_metrics_hv, avg_m_bNB_hv])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hist_plot(data, name):\n",
    "    x = np.arange(data.shape[0])\n",
    "    dx = (np.arange(data.shape[1])-data.shape[1]/2.)/(data.shape[1]+2.)\n",
    "    d = 1./(data.shape[1]+2.)\n",
    "\n",
    "\n",
    "    fig, ax=plt.subplots()\n",
    "    labels = ['Alg1','Alg2','Alg3','Alg4','bNB']\n",
    "    x_axis = np.arange(0, 5, 1)\n",
    "    for i in range(data.shape[1]):\n",
    "        ax.bar(x+dx[i],data[:,i], width=d)#, label=\"label {}\".format(labels[i]))\n",
    "    plt.xticks(x_axis,labels)\n",
    "    plt.title(name)\n",
    "    plt.legend(['Accuracy score', 'ROC AUC', 'Precision','Recall'],bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist_plot(avg_metrics_ttt_full, name= 'tic tac toe dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist_plot(avg_metrics_hv_full, name= 'house votes dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}